{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my Histopathologic Cancer Detection Neural Network (Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the library versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.15.0\n",
      "Python: 3.10.11\n",
      "numpy: 1.24.3\n",
      "pandas: 2.1.4\n",
      "sklearn version: 1.2.2\n",
      "sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']\n",
      "matplotlib: 3.8.2\n",
      "seaborn: 0.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tensorflow:\", tf.__version__)\n",
    "\n",
    "# import keras\n",
    "# print(\"keras:\", keras.__version__)\n",
    "\n",
    "# import kerastuner as kt\n",
    "# print(\"kerastuner:\", kt.__version__)\n",
    "\n",
    "# import keras_tuner as kt2\n",
    "# print(\"keras_tuner:\", kt2.__version__)\n",
    "\n",
    "import platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "import numpy as np\n",
    "print(\"numpy:\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn path:\", sklearn.__path__)\n",
    "\n",
    "import matplotlib\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "# Tensorflow: 2.15.0\n",
    "# kerastuner: 1.0.5\n",
    "# keras_tuner: 1.3.5\n",
    "# Python: 3.10.11\n",
    "# numpy: 1.24.3\n",
    "# pandas: 2.1.4\n",
    "# sklearn version: 1.2.2\n",
    "# sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']\n",
    "# matplotlib: 3.8.2\n",
    "# seaborn: 0.13.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Global random seed to make sure we can replicate any model that we create (no randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition pre-processing practices:\n",
    "\n",
    "1. **Rescaling**: Image pixel values usually range from 0 to 255. Rescaling these values to a range of 0 to 1 by dividing each pixel by 255 is a common practice. This helps to stabilize and speed up the learning process.\n",
    "\n",
    "2. **Resizing**: Deep learning models require the input dimensions to be uniform. Resizing all images to a predetermined size is essential, especially if the original dataset contains images of various dimensions.\n",
    "\n",
    "3. **Normalization**: Beyond just rescaling, you might want to normalize the image data. This can include subtracting the mean and dividing by the standard deviation across each channel. If you have a pre-trained model, you would use the normalization statistics (mean and standard deviation) from the dataset on which the model was trained.\n",
    "\n",
    "4. **Data Augmentation**: To increase the diversity of your dataset and prevent overfitting, you can apply random transformations like rotation, shifting, flipping, zooming, and shearing. These transformations generate new training samples from the original ones by altering them slightly.\n",
    "\n",
    "5. **Color Space Conversions**: Sometimes, converting images to different color spaces (e.g., from RGB to grayscale, HSV, LAB, etc.) can help the model learn more robust features, depending on the task.\n",
    "\n",
    "6. **Image Denoising**: If the images are noisy, applying denoising algorithms can help to remove noise and improve model accuracy.\n",
    "\n",
    "7. **Edge Detection**: In certain applications, particularly those involving shape analysis, edge detection filters may be applied to highlight the edges within images.\n",
    "\n",
    "8. **Masking and Cropping**: If there are regions in the images that are not relevant to the analysis, you might want to mask or crop these regions to focus the model on the important parts of the image.\n",
    "\n",
    "9. **Histogram Equalization**: This can enhance the contrast in images, which can be beneficial if you have a dataset with varying lighting conditions.\n",
    "\n",
    "10. **Centering and Standardization**: Similar to normalization, centering the data by subtracting the mean image (computed over the training set) and standardizing, so the variance of the pixels is reduced, can be beneficial.\n",
    "\n",
    "11. **Handling Class Imbalance**: If your dataset has a class imbalance, techniques such as class weighting, oversampling the minority class, or undersampling the majority class can be considered.\n",
    "\n",
    "In practice, preprocessing steps are often determined experimentally. You might start with a simple preprocessing pipeline (like just rescaling and resizing) and then iteratively add steps that improve your model performance. It's also important to note that if you're using a pre-trained model, you should preprocess your data in the same way the original model was trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if each image has the same dimensions since that's important for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# def check_image_dimensions(directory):\n",
    "#     image_sizes = set()\n",
    "#     for img_name in os.listdir(directory):\n",
    "#         img_path = os.path.join(directory, img_name)\n",
    "#         with Image.open(img_path) as img:\n",
    "#             # Get image size\n",
    "#             size = img.size\n",
    "#             image_sizes.add(size)\n",
    "            \n",
    "#             # # If more than one size is found, we can stop checking\n",
    "#             # if len(image_sizes) > 1:\n",
    "#             #     break\n",
    "    \n",
    "#     if len(image_sizes) == 1:\n",
    "#         print(f\"For the {directory} directory, all images are of the same dimension: {image_sizes.pop()}\")\n",
    "#     else:\n",
    "#         print(f\"For the {directory} directory, different dimensions found: {image_sizes}\")\n",
    "\n",
    "# # Use it on the train and test data only if this code segment was never ran in this coding session:\n",
    "# if 'checked_image_dimensions' not in globals():\n",
    "#     # Use it on the train and test data:\n",
    "#     check_image_dimensions('train')\n",
    "#     check_image_dimensions('test')\n",
    "#     checked_image_dimensions = True\n",
    "\n",
    "# # For the train directory, all images are of the same dimension: (96, 96)\n",
    "# # For the test directory, all images are of the same dimension: (96, 96)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you need to, call the resize_images functions to ensure each image is the same dimension but make sure you are not distorting the images. In order to do this, you need to make sure all the original images have the same aspect ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    " \n",
    "\n",
    "def resize_images(directory, size=(128, 128)): \n",
    "    for img_name in os.listdir(directory):\n",
    "        img_path = os.path.join(directory, img_name)\n",
    "        with Image.open(img_path) as img:\n",
    "            new_img = img.resize(size)\n",
    "            new_img.save(img_path)\n",
    "\n",
    "# Use it on the train and test data if needed, and change the size argument as you need:\n",
    "            \n",
    "# resize_images('train', size=(128, 128))\n",
    "# resize_images('test', size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a training, validation, testing sets \n",
    "Make sure to do this before using data augmentation like ImageDataGenerator(). It's hard to split the data into train-validation-test after using ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the base directory to the current directory\n",
    "base_dir = ''\n",
    "\n",
    "# Directory for train\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "\n",
    "# Load the labels\n",
    "labels = pd.read_csv(os.path.join(base_dir, 'train_labels.csv'))\n",
    "\n",
    "# Convert the 'label' column to strings\n",
    "labels['label'] = labels['label'].astype(str)\n",
    "\n",
    "# Add the full path to the image files, and create a new column called \"path\" inside the label dataframe to store these paths to images\n",
    "labels['path'] = labels['id'].apply(lambda x: os.path.join(train_dir, f\"{x}.tif\"))\n",
    "\n",
    "# Split the labels dataframe into train, validation, and test sets into a 70/15/15 ratio\n",
    "train_df, test_df = train_test_split(labels, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "train_df.to_csv('training_df_labels.csv', index=False)\n",
    "val_df.to_csv('valid_df_labels.csv', index=False)\n",
    "test_df.to_csv('testing_df_labels.csv', index=False)\n",
    "\n",
    "# Now we have a dataframe for train, val, and test which contains the data of their path, label, and id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Keras ImageDataGenerator() on Train/Validation/Test split and also crop 32x32px center\n",
    "The ImageDataGenerator not only helps you load images from the disk but also allows you to perform **data augmentation**, which is a technique to increase the diversity of your training set by applying random transformations (like rotation, zoom, flips, etc.) to the images. This is very useful to prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure to change the \"target_size\" argument of the train_datagen.flow_from_dataframe() function as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 154017 validated image filenames belonging to 2 classes.\n",
      "Found 33004 validated image filenames belonging to 2 classes.\n",
      "Found 33004 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "#This is a function to crop the image to focus on the 32x32px center of the image. We will call this function in the ImageDataGenerator() function\n",
    "def crop_center(img): \n",
    "    y, x, _ = img.shape\n",
    "    startx = x//2 - (32//2)\n",
    "    starty = y//2 - (32//2)    \n",
    "    return img[starty:starty+32, startx:startx+32, :]\n",
    "\n",
    "\n",
    "\n",
    "# Creating an instance of the ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the image pixel values to [0,1]\n",
    "    preprocessing_function = crop_center  # call the crop function on each image\n",
    "\n",
    "    # Potential data augmentation techniques that won't affect the 32x32px center\n",
    "    #brightness_range=[0.8, 1.2], \n",
    "    #channel_shift_range=20, \n",
    "\n",
    "    # I removed these transformations for the data augmentation since this project involves detecting tumor tissue in the center 32x32px region so I can't be doing zooming and other transformations for this project specifically\n",
    "\n",
    "    # rotation_range=40,  # Random rotations\n",
    "    # width_shift_range=0.2,  # Random horizontal shifts\n",
    "    # height_shift_range=0.2,  # Random vertical shifts\n",
    "    # shear_range=0.2,  # Shear transformations\n",
    "    # zoom_range=0.2,  # Random zoom\n",
    "    # horizontal_flip=True,  # Random horizontal flips\n",
    "    # fill_mode='nearest'  # Strategy for filling in new pixels\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function = crop_center)  # call the crop function on each image\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function = crop_center) # call the crop function on each image\n",
    "\n",
    "\n",
    "\n",
    "# Flow from dataframe method to load images using the dataframe\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df, # Use the training dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),  # The dimensions to which all images found will be resized. Change this as needed\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary', # means that the labels are binary labels\n",
    "    batch_size=32,\n",
    "    shuffle=True, # This might introduce randomness if set to true, but if it's false, the it might lead to overfitting. So it's best to just save the neural network to ensure no randomness\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df, # Use the validation dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df, # Use the testing dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# After setting this up, you can use train_generator as the input to the fit or fit_generator method of your Keras model, \n",
    "# which will load images in batches and train your model on them.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want, you can check the image shape and see a visualization of the pictures below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Get a batch of images\n",
    "# images, labels = next(train_generator)\n",
    "\n",
    "# # The images should now be a numpy array. Check its shape:\n",
    "# print(images.shape)  # Should be (batch_size, target_size[0], target_size[1], 3)\n",
    "\n",
    "# # Plot the first few images\n",
    "# for i in range(5):  # Change this value to see more images\n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.imshow(images[i])\n",
    "#     plt.title(f'Label: {labels[i]}') \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is another function which is able to crop images but you have to manually call this function on each image in order to crop, so I just used the ImageDataGenerator() method instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_center(img):\n",
    "    width, height = img.size\n",
    "    new_width, new_height = 32, 32\n",
    "\n",
    "    left = (width - new_width)/2\n",
    "    top = (height - new_height)/2\n",
    "    right = (width + new_width)/2\n",
    "    bottom = (height + new_height)/2\n",
    "\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "\n",
    "#The code below is for you to visually see the cropped images\n",
    "\n",
    "# # Get a batch of images\n",
    "# images, labels = next(train_generator)\n",
    "\n",
    "# # The images should now be a numpy array. Check its shape:\n",
    "# print(images.shape)  # Should be (batch_size, target_size[0], target_size[1], 3)\n",
    "\n",
    "# # Crop and plot the first few images\n",
    "# for i in range(5):  # Change this value to see more images\n",
    "#     img = Image.fromarray((images[i] * 255).astype(np.uint8))  # Convert to PIL Image\n",
    "#     cropped_img = crop_center(img)\n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.imshow(cropped_img)\n",
    "#     plt.title(f'Label: {labels[i]}') \n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ImageDataGenerator on the actual test data (from the test directory, not the testing data from the train/valid/test split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57458 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the base directory to the current directory\n",
    "base_dir = ''\n",
    "\n",
    "# Directory for test\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Get the list of test image filenames\n",
    "test_filenames = os.listdir(test_dir)\n",
    "\n",
    "# Create a DataFrame with 'id' and 'path' columns\n",
    "df_test = pd.DataFrame({\n",
    "    'id': [filename.split('.')[0] for filename in test_filenames],\n",
    "    'path': [os.path.join(test_dir, filename) for filename in test_filenames]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "#This is a function to crop the image to focus on the 32x32px center of the image. We will call this function in the ImageDataGenerator() function\n",
    "def crop_center(img): \n",
    "    y, x, _ = img.shape\n",
    "    startx = x//2 - (32//2)\n",
    "    starty = y//2 - (32//2)    \n",
    "    return img[starty:starty+32, startx:startx+32, :]\n",
    "\n",
    "# Create a data generator for the test data\n",
    "real_test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function = crop_center)\n",
    "\n",
    "real_test_generator = real_test_datagen.flow_from_dataframe(\n",
    "        dataframe = df_test,\n",
    "        x_col=\"path\",\n",
    "        y_col=None,  # We don't have labels for the test data\n",
    "        target_size=(32, 32),\n",
    "        batch_size=32, # Change the batch size as needed\n",
    "        class_mode=None,  # We don't have labels for the test data\n",
    "        color_mode = \"rgb\",\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, it's time to create my first model. This is Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_13220\\751830747.py:19: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, validation_data=val_generator, epochs=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4814/4814 [==============================] - 189s 39ms/step - loss: 0.4595 - accuracy: 0.7881 - val_loss: 0.4646 - val_accuracy: 0.7834\n",
      "Epoch 2/10\n",
      "4814/4814 [==============================] - 105s 22ms/step - loss: 0.3977 - accuracy: 0.8229 - val_loss: 0.3794 - val_accuracy: 0.8352\n",
      "Epoch 3/10\n",
      "4814/4814 [==============================] - 103s 21ms/step - loss: 0.3756 - accuracy: 0.8338 - val_loss: 0.3673 - val_accuracy: 0.8420\n",
      "Epoch 4/10\n",
      "4814/4814 [==============================] - 128s 27ms/step - loss: 0.3620 - accuracy: 0.8411 - val_loss: 0.3622 - val_accuracy: 0.8407\n",
      "Epoch 5/10\n",
      "4814/4814 [==============================] - 170s 35ms/step - loss: 0.3513 - accuracy: 0.8469 - val_loss: 0.3540 - val_accuracy: 0.8464\n",
      "Epoch 6/10\n",
      "4814/4814 [==============================] - 157s 33ms/step - loss: 0.3416 - accuracy: 0.8517 - val_loss: 0.3608 - val_accuracy: 0.8412\n",
      "Epoch 7/10\n",
      "4814/4814 [==============================] - 144s 30ms/step - loss: 0.3319 - accuracy: 0.8559 - val_loss: 0.3695 - val_accuracy: 0.8379\n",
      "Epoch 8/10\n",
      "4814/4814 [==============================] - 118s 25ms/step - loss: 0.3227 - accuracy: 0.8607 - val_loss: 0.3492 - val_accuracy: 0.8475\n",
      "Epoch 9/10\n",
      "4814/4814 [==============================] - 117s 24ms/step - loss: 0.3133 - accuracy: 0.8651 - val_loss: 0.3561 - val_accuracy: 0.8444\n",
      "Epoch 10/10\n",
      "4814/4814 [==============================] - 95s 20ms/step - loss: 0.3042 - accuracy: 0.8690 - val_loss: 0.3402 - val_accuracy: 0.8557\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.3436 - accuracy: 0.8527\n",
      "Test accuracy: 0.8527148365974426\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# 1. Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#3. Fit the model\n",
    "model.fit_generator(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "#4. Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "#Took almost 23 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))`: This line creates a 2D convolution layer. Convolution layers are the major building blocks used in convolutional neural networks. A convolution layer transforms an input volume into an output volume of different size, as specified by the parameters of the layer. In this case, the layer will output 32 different feature maps, each one representing a different learned feature. The `(3, 3)` parameter specifies the size of the filters that will be learned, and `relu` is the activation function that will be applied element-wise to the output. The `input_shape=(32, 32, 3)` parameter specifies the shape of the input data: images of size 32x32 pixels with 3 color channels (red, green, blue).\n",
    "\n",
    "2. `MaxPooling2D((2, 2))`: This line creates a max pooling layer, which is used to reduce the spatial dimensions of the output volume from the previous layer. It does this by taking the maximum value over a 2x2 window. This helps to make the model more translation invariant and to reduce computation.\n",
    "\n",
    "3. `Conv2D(64, (3, 3), activation='relu')`: This is another convolution layer, similar to the first one. This layer will learn 64 filters. The size of the filters is again 3x3 pixels, and the activation function is ReLU.\n",
    "\n",
    "4. `MaxPooling2D((2, 2))`: This is another max pooling layer, similar to the first one. It again reduces the spatial dimensions of the output volume from the previous layer.\n",
    "\n",
    "5. `Flatten()`: This layer flattens the output from the previous layer into a one-dimensional vector. This is necessary because the next layer (a dense layer) expects its input to be a vector, not a multi-dimensional array.\n",
    "\n",
    "6. `Dense(64, activation='relu')`: This is a fully connected layer, also known as a dense layer. Each neuron in a dense layer receives input from all the neurons in the previous layer, hence they are \"fully connected\". This layer has 64 neurons and uses the ReLU activation function.\n",
    "\n",
    "7. `Dense(1, activation='sigmoid')`: This is the output layer of the model. It's another dense layer, and it has just one neuron because this is a binary classification problem (assuming your labels are 0 and 1). The sigmoid activation function is used to squash the output of the neuron to a value between 0 and 1, representing the probability that the image belongs to class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796/1796 [==============================] - 51s 28ms/step\n",
      "['test\\\\00006537328c33e284c973d7b39d340809f7271b.tif', 'test\\\\0000ec92553fda4ce39889f9226ace43cae3364e.tif', 'test\\\\00024a6dee61f12f7856b0fc6be20bc7a48ba3d2.tif', 'test\\\\000253dfaa0be9d0d100283b22284ab2f6b643f6.tif', 'test\\\\000270442cc15af719583a8172c87cd2bd9c7746.tif']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(real_test_generator, steps=len(real_test_generator), verbose=1)\n",
    "\n",
    "# Get filenames (ordered list of image file names)\n",
    "filenames = real_test_generator.filenames\n",
    "print(filenames[:5])\n",
    "\n",
    "# Get the actual predictions, not the probabilities\n",
    "# If your model is a binary classifier, this will convert the probabilities into class predictions\n",
    "predicted_classes = [1 if prob > 0.5 else 0 for prob in predictions]\n",
    "\n",
    "# Create a DataFrame with filenames and predicted classes\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(filename).split('.')[0] for filename in filenames],  # Extract the id from the filename\n",
    "    'label': predicted_classes\n",
    "})\n",
    "\n",
    "# Save DataFrame to csv\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")\n",
    "\n",
    "# Took 51 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGE THE FIT METHOD FROM fit_generator() to fit()\n",
    "## LEARN HOW TO SAVE MODEL AND THE HYPERPARAMETERS. LEARN HOW TO PRINT OUT THE MOST IMPORTANT INFO OF THE MODEL, LIKE I DID FOR THE TITANIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Maybe try GrayScale conversation\n",
    "### 2. Try Image cropping for 32x32px or 33x33px or no image cropping at all\n",
    "### 3. Try histogram equalization (part of data preprocessing)\n",
    "### 4. Find a way to make the images less blurry or find a way to make it not lose any pixels since each pixel is important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
