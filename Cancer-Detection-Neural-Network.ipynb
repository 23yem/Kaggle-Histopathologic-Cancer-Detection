{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my Histopathologic Cancer Detection Neural Network (Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the library versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "tensorflow: 2.15.0\n",
      "Python: 3.10.11\n",
      "numpy: 1.24.3\n",
      "pandas: 2.1.4\n",
      "sklearn version: 1.2.2\n",
      "sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']\n",
      "matplotlib: 3.8.2\n",
      "seaborn: 0.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tensorflow:\", tf.__version__)\n",
    "\n",
    "# import keras\n",
    "# print(\"keras:\", keras.__version__)\n",
    "\n",
    "# import kerastuner as kt\n",
    "# print(\"kerastuner:\", kt.__version__)\n",
    "\n",
    "# import keras_tuner as kt2\n",
    "# print(\"keras_tuner:\", kt2.__version__)\n",
    "\n",
    "import platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "import numpy as np\n",
    "print(\"numpy:\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn path:\", sklearn.__path__)\n",
    "\n",
    "import matplotlib\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "# Tensorflow: 2.15.0\n",
    "# kerastuner: 1.0.5\n",
    "# keras_tuner: 1.3.5\n",
    "# Python: 3.10.11\n",
    "# numpy: 1.24.3\n",
    "# pandas: 2.1.4\n",
    "# sklearn version: 1.2.2\n",
    "# sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']\n",
    "# matplotlib: 3.8.2\n",
    "# seaborn: 0.13.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Global random seed to make sure we can replicate any model that we create (no randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition pre-processing practices:\n",
    "\n",
    "1. **Rescaling**: Image pixel values usually range from 0 to 255. Rescaling these values to a range of 0 to 1 by dividing each pixel by 255 is a common practice. This helps to stabilize and speed up the learning process.\n",
    "\n",
    "2. **Resizing**: Deep learning models require the input dimensions to be uniform. Resizing all images to a predetermined size is essential, especially if the original dataset contains images of various dimensions.\n",
    "\n",
    "3. **Normalization**: Beyond just rescaling, you might want to normalize the image data. This can include subtracting the mean and dividing by the standard deviation across each channel. If you have a pre-trained model, you would use the normalization statistics (mean and standard deviation) from the dataset on which the model was trained.\n",
    "\n",
    "4. **Data Augmentation**: To increase the diversity of your dataset and prevent overfitting, you can apply random transformations like rotation, shifting, flipping, zooming, and shearing. These transformations generate new training samples from the original ones by altering them slightly.\n",
    "\n",
    "5. **Color Space Conversions**: Sometimes, converting images to different color spaces (e.g., from RGB to grayscale, HSV, LAB, etc.) can help the model learn more robust features, depending on the task.\n",
    "\n",
    "6. **Image Denoising**: If the images are noisy, applying denoising algorithms can help to remove noise and improve model accuracy.\n",
    "\n",
    "7. **Edge Detection**: In certain applications, particularly those involving shape analysis, edge detection filters may be applied to highlight the edges within images.\n",
    "\n",
    "8. **Masking and Cropping**: If there are regions in the images that are not relevant to the analysis, you might want to mask or crop these regions to focus the model on the important parts of the image.\n",
    "\n",
    "9. **Histogram Equalization**: This can enhance the contrast in images, which can be beneficial if you have a dataset with varying lighting conditions.\n",
    "\n",
    "10. **Centering and Standardization**: Similar to normalization, centering the data by subtracting the mean image (computed over the training set) and standardizing, so the variance of the pixels is reduced, can be beneficial.\n",
    "\n",
    "11. **Handling Class Imbalance**: If your dataset has a class imbalance, techniques such as class weighting, oversampling the minority class, or undersampling the majority class can be considered.\n",
    "\n",
    "In practice, preprocessing steps are often determined experimentally. You might start with a simple preprocessing pipeline (like just rescaling and resizing) and then iteratively add steps that improve your model performance. It's also important to note that if you're using a pre-trained model, you should preprocess your data in the same way the original model was trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if each image has the same dimensions since that's important for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# def check_image_dimensions(directory):\n",
    "#     image_sizes = set()\n",
    "#     for img_name in os.listdir(directory):\n",
    "#         img_path = os.path.join(directory, img_name)\n",
    "#         with Image.open(img_path) as img:\n",
    "#             # Get image size\n",
    "#             size = img.size\n",
    "#             image_sizes.add(size)\n",
    "            \n",
    "#             # # If more than one size is found, we can stop checking\n",
    "#             # if len(image_sizes) > 1:\n",
    "#             #     break\n",
    "    \n",
    "#     if len(image_sizes) == 1:\n",
    "#         print(f\"For the {directory} directory, all images are of the same dimension: {image_sizes.pop()}\")\n",
    "#     else:\n",
    "#         print(f\"For the {directory} directory, different dimensions found: {image_sizes}\")\n",
    "\n",
    "# # Use it on the train and test data only if this code segment was never ran in this coding session:\n",
    "# if 'checked_image_dimensions' not in globals():\n",
    "#     # Use it on the train and test data:\n",
    "#     check_image_dimensions('train')\n",
    "#     check_image_dimensions('test')\n",
    "#     checked_image_dimensions = True\n",
    "\n",
    "# # For the train directory, all images are of the same dimension: (96, 96)\n",
    "# # For the test directory, all images are of the same dimension: (96, 96)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you need to, call the resize_images functions to ensure each image is the same dimension but make sure you are not distorting the images. In order to do this, you need to make sure all the original images have the same aspect ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a warning: this will resize the original images. A better way to do this is in-memory processing, where you only save images in the disk for temporary use, like a variable in python. So, you can just create the function, give it to the TensorFlow ImageDataGenerator, and then it will call the resize function on all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    " \n",
    "\n",
    "def resize_images(directory, size=(128, 128)): \n",
    "    for img_name in os.listdir(directory):\n",
    "        img_path = os.path.join(directory, img_name)\n",
    "        with Image.open(img_path) as img:\n",
    "            new_img = img.resize(size)\n",
    "            new_img.save(img_path)\n",
    "\n",
    "# Use it on the train and test data if needed, and change the size argument as you need:\n",
    "            \n",
    "# resize_images('train', size=(128, 128))\n",
    "# resize_images('test', size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a training, validation, testing sets \n",
    "Make sure to do this before using data augmentation like ImageDataGenerator(). It's hard to split the data into train-validation-test after using ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the base directory to the current directory\n",
    "base_dir = ''\n",
    "\n",
    "# Directory for train\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "\n",
    "# Load the labels\n",
    "labels = pd.read_csv(os.path.join(base_dir, 'train_labels.csv'))\n",
    "\n",
    "# Convert the 'label' column to strings\n",
    "labels['label'] = labels['label'].astype(str)\n",
    "\n",
    "# Add the full path to the image files, and create a new column called \"path\" inside the label dataframe to store these paths to images\n",
    "labels['path'] = labels['id'].apply(lambda x: os.path.join(train_dir, f\"{x}.tif\"))\n",
    "\n",
    "# Split the labels dataframe into train, validation, and test sets into a 70/15/15 ratio\n",
    "train_df, test_df = train_test_split(labels, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "train_df.to_csv('training_df_labels.csv', index=False)\n",
    "val_df.to_csv('valid_df_labels.csv', index=False)\n",
    "test_df.to_csv('testing_df_labels.csv', index=False)\n",
    "\n",
    "# Now we have a dataframe for train, val, and test which contains the data of their path, label, and id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Keras ImageDataGenerator() on Train/Validation/Test split and also crop 32x32px center\n",
    "The ImageDataGenerator not only helps you load images from the disk but also allows you to perform **data augmentation**, which is a technique to increase the diversity of your training set by applying random transformations (like rotation, zoom, flips, etc.) to the images. This is very useful to prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure to change the \"target_size\" argument of the train_datagen.flow_from_dataframe() function as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 154017 validated image filenames belonging to 2 classes.\n",
      "Found 33004 validated image filenames belonging to 2 classes.\n",
      "Found 33004 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "#This is a function to crop the image to focus on the 32x32px center of the image. We will call this function in the ImageDataGenerator() function\n",
    "def crop_center(img): \n",
    "    y, x, _ = img.shape\n",
    "    startx = x//2 - (32//2)\n",
    "    starty = y//2 - (32//2)    \n",
    "    return img[starty:starty+32, startx:startx+32, :]\n",
    "\n",
    "\n",
    "\n",
    "# Creating an instance of the ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the image pixel values to [0,1]\n",
    "    preprocessing_function = crop_center  # call the crop function on each image\n",
    "\n",
    "    # Potential data augmentation techniques that won't affect the 32x32px center\n",
    "    #brightness_range=[0.8, 1.2], \n",
    "    #channel_shift_range=20, \n",
    "\n",
    "    # I removed these transformations for the data augmentation since this project involves detecting tumor tissue in the center 32x32px region so I can't be doing zooming and other transformations for this project specifically\n",
    "\n",
    "    # rotation_range=40,  # Random rotations\n",
    "    # width_shift_range=0.2,  # Random horizontal shifts\n",
    "    # height_shift_range=0.2,  # Random vertical shifts\n",
    "    # shear_range=0.2,  # Shear transformations\n",
    "    # zoom_range=0.2,  # Random zoom\n",
    "    # horizontal_flip=True,  # Random horizontal flips\n",
    "    # fill_mode='nearest'  # Strategy for filling in new pixels\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function = crop_center)  # call the crop function on each image\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function = crop_center) # call the crop function on each image\n",
    "\n",
    "\n",
    "\n",
    "# Flow from dataframe method to load images using the dataframe\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df, # Use the training dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),  # The dimensions to which all images found will be resized. Change this as needed\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary', # means that the labels are binary labels\n",
    "    batch_size=32,\n",
    "    shuffle=True, # This might introduce randomness if set to true, but if it's false, the it might lead to overfitting. So it's best to just save the neural network to ensure no randomness\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df, # Use the validation dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df, # Use the testing dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# After setting this up, you can use train_generator as the input to the fit or fit_generator method of your Keras model, \n",
    "# which will load images in batches and train your model on them.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want, you can check the image shape and see a visualization of the pictures below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Get a batch of images\n",
    "# images, labels = next(train_generator)\n",
    "\n",
    "# # The images should now be a numpy array. Check its shape:\n",
    "# print(images.shape)  # Should be (batch_size, target_size[0], target_size[1], 3)\n",
    "\n",
    "# # Plot the first few images\n",
    "# for i in range(5):  # Change this value to see more images\n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.imshow(images[i])\n",
    "#     plt.title(f'Label: {labels[i]}') \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is another function which is able to crop images but you have to manually call this function on each image in order to crop, so I just used the ImageDataGenerator() method instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_center(img):\n",
    "    width, height = img.size\n",
    "    new_width, new_height = 32, 32\n",
    "\n",
    "    left = (width - new_width)/2\n",
    "    top = (height - new_height)/2\n",
    "    right = (width + new_width)/2\n",
    "    bottom = (height + new_height)/2\n",
    "\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "\n",
    "#The code below is for you to visually see the cropped images\n",
    "\n",
    "# # Get a batch of images\n",
    "# images, labels = next(train_generator)\n",
    "\n",
    "# # The images should now be a numpy array. Check its shape:\n",
    "# print(images.shape)  # Should be (batch_size, target_size[0], target_size[1], 3)\n",
    "\n",
    "# # Crop and plot the first few images\n",
    "# for i in range(5):  # Change this value to see more images\n",
    "#     img = Image.fromarray((images[i] * 255).astype(np.uint8))  # Convert to PIL Image\n",
    "#     cropped_img = crop_center(img)\n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.imshow(cropped_img)\n",
    "#     plt.title(f'Label: {labels[i]}') \n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ImageDataGenerator on the actual test data (from the test directory, not the testing data from the train/valid/test split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57458 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the base directory to the current directory\n",
    "base_dir = ''\n",
    "\n",
    "# Directory for test\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Get the list of test image filenames\n",
    "test_filenames = os.listdir(test_dir)\n",
    "\n",
    "# Create a DataFrame with 'id' and 'path' columns\n",
    "df_test = pd.DataFrame({\n",
    "    'id': [filename.split('.')[0] for filename in test_filenames],\n",
    "    'path': [os.path.join(test_dir, filename) for filename in test_filenames]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "#This is a function to crop the image to focus on the 32x32px center of the image. We will call this function in the ImageDataGenerator() function\n",
    "def crop_center(img): \n",
    "    y, x, _ = img.shape\n",
    "    startx = x//2 - (32//2)\n",
    "    starty = y//2 - (32//2)    \n",
    "    return img[starty:starty+32, startx:startx+32, :]\n",
    "\n",
    "# Create a data generator for the test data\n",
    "real_test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function = crop_center)\n",
    "\n",
    "real_test_generator = real_test_datagen.flow_from_dataframe(\n",
    "        dataframe = df_test,\n",
    "        x_col=\"path\",\n",
    "        y_col=None,  # We don't have labels for the test data\n",
    "        target_size=(32, 32),\n",
    "        batch_size=32, # Change the batch size as needed\n",
    "        class_mode=None,  # We don't have labels for the test data\n",
    "        color_mode = \"rgb\",\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, it's time to create my first model. This is Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# # 1. Define the model architecture\n",
    "# model = Sequential([\n",
    "#     Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Conv2D(64, (3, 3), activation='relu'),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Flatten(),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# #2. Compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# #3. Fit the model\n",
    "# model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# #4. Evaluate the model\n",
    "# loss, accuracy = model.evaluate(test_generator)\n",
    "# print('Test accuracy:', accuracy)\n",
    "\n",
    "# #Took almost 23 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))`: This line creates a 2D convolution layer. Convolution layers are the major building blocks used in convolutional neural networks. A convolution layer transforms an input volume into an output volume of different size, as specified by the parameters of the layer. In this case, the layer will output 32 different feature maps, each one representing a different learned feature. The `(3, 3)` parameter specifies the size of the filters that will be learned, and `relu` is the activation function that will be applied element-wise to the output. The `input_shape=(32, 32, 3)` parameter specifies the shape of the input data: images of size 32x32 pixels with 3 color channels (red, green, blue).\n",
    "\n",
    "2. `MaxPooling2D((2, 2))`: This line creates a max pooling layer, which is used to reduce the spatial dimensions of the output volume from the previous layer. It does this by taking the maximum value over a 2x2 window. This helps to make the model more translation invariant and to reduce computation.\n",
    "\n",
    "3. `Conv2D(64, (3, 3), activation='relu')`: This is another convolution layer, similar to the first one. This layer will learn 64 filters. The size of the filters is again 3x3 pixels, and the activation function is ReLU.\n",
    "\n",
    "4. `MaxPooling2D((2, 2))`: This is another max pooling layer, similar to the first one. It again reduces the spatial dimensions of the output volume from the previous layer.\n",
    "\n",
    "5. `Flatten()`: This layer flattens the output from the previous layer into a one-dimensional vector. This is necessary because the next layer (a dense layer) expects its input to be a vector, not a multi-dimensional array.\n",
    "\n",
    "6. `Dense(64, activation='relu')`: This is a fully connected layer, also known as a dense layer. Each neuron in a dense layer receives input from all the neurons in the previous layer, hence they are \"fully connected\". This layer has 64 neurons and uses the ReLU activation function.\n",
    "\n",
    "7. `Dense(1, activation='sigmoid')`: This is the output layer of the model. It's another dense layer, and it has just one neuron because this is a binary classification problem (assuming your labels are 0 and 1). The sigmoid activation function is used to squash the output of the neuron to a value between 0 and 1, representing the probability that the image belongs to class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying information about Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model Summary:\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# # Get model configuration\n",
    "# config = model.get_config()\n",
    "# print(config)\n",
    "\n",
    "\n",
    "# # Get model weights\n",
    "# weights = model.get_weights()\n",
    "# for i, weight in enumerate(weights):\n",
    "#     print(f\"Weights of layer {i}: {weight}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Plot model architecture (visual representation)\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model, to_file='model1_plot.png', show_shapes=True, show_layer_names=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(real_test_generator, steps=len(real_test_generator), verbose=1)\n",
    "\n",
    "# model.save(\"model1_saved.h5\")\n",
    "\n",
    "# # Get filenames (ordered list of image file names)\n",
    "# filenames = real_test_generator.filenames\n",
    "# print(filenames[:5])\n",
    "\n",
    "# # Get the actual predictions, not the probabilities\n",
    "# # If your model is a binary classifier, this will convert the probabilities into class predictions\n",
    "# predicted_classes = [1 if prob > 0.5 else 0 for prob in predictions]\n",
    "\n",
    "# # Create a DataFrame with filenames and predicted classes\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'id': [os.path.basename(filename).split('.')[0] for filename in filenames],  # Extract the id from the filename\n",
    "#     'label': predicted_classes\n",
    "# })\n",
    "\n",
    "# # Save DataFrame to csv\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "# print(\"Your submission was successfully saved!\")\n",
    "\n",
    "# # Took 51 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Model 3 using Keras Tuner for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tuning a Convolutional Neural Network (CNN) using Keras Tuner, there are several hyperparameters that are commonly experimented with:\n",
    "\n",
    "1. **Number of Convolutional Layers:** The number of convolutional layers in the network can significantly affect the model's performance. More layers allow the model to learn more complex features, but can also lead to overfitting and longer training times.\n",
    "\n",
    "2. **Number of Filters in Convolutional Layers:** The number of filters in each convolutional layer is another important hyperparameter. More filters allow the model to learn more complex features, but can also lead to overfitting and longer training times.\n",
    "\n",
    "3. **Kernel Size in Convolutional Layers:** The size of the kernels used in the convolutional layers can affect the scale of the features that the model can learn. Smaller kernels allow the model to learn more local features, while larger kernels allow it to learn more global features.\n",
    "\n",
    "4. **Pooling Size in Pooling Layers:** The size of the pooling operation can affect the amount of down-sampling performed in the network. Larger pooling sizes result in more down-sampling, which can help to reduce overfitting and computational cost, but can also cause the model to lose important information.\n",
    "\n",
    "5. **Number of Units in Dense Layers:** The number of units in the dense layers can affect the capacity of the model. More units allow the model to learn more complex representations, but can also lead to overfitting and longer training times.\n",
    "\n",
    "6. **Learning Rate:** The learning rate controls the step size during optimization. A learning rate that is too high can cause the model to converge too quickly to a suboptimal solution, while a learning rate that is too low can cause the model to converge too slowly.\n",
    "\n",
    "7. **Optimizer:** Different optimizers can converge at different rates. Common choices include SGD, Adam, and RMSprop.\n",
    "\n",
    "8. **Activation Functions:** The choice of activation function can affect the model's ability to learn complex patterns. Common choices include ReLU, sigmoid, and tanh.\n",
    "\n",
    "Remember, the best hyperparameters can vary depending on the specific problem and dataset, so it's always a good idea to experiment with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from kerastuner.tuners import Hyperband\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Tune the number of filters in the first Conv2D layer\n",
    "#     hp_filters = hp.Int('conv_1_filters', min_value = 32, max_value = 128, step = 16)\n",
    "#     model.add(Conv2D(filters = hp_filters, kernel_size = (3,3), activation = 'relu', input_shape = (32,32,3)))\n",
    "    \n",
    "#     model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "#     for i in range(hp.Int('num_layers', 2, 5)):\n",
    "#         model.add(Conv2D(filters = hp.Int(f'conv_{i+2}_filters', min_value = 32, max_value = 128, step = 16), \n",
    "#                                       kernel_size = hp.Choice(f'conv_{i+2}_kernel', values = [2,3]), \n",
    "#                                       activation = 'relu'))\n",
    "#         model.add(MaxPooling2D(pool_size=hp.Choice(f'pool_{i+2}_size', values = [2,3])))\n",
    "    \n",
    "#     model.add(Flatten())\n",
    "    \n",
    "#     model.add(Dense(units = hp.Int('dense_1_units', min_value = 32, max_value = 512, step = 32), activation = 'relu'))\n",
    "    \n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     # Tune the learning rate for the optimizer \n",
    "#     hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "\n",
    "#     model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "#                 loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "#                 metrics = ['accuracy'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Add the callback to the tuner\n",
    "# tuner = Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     executions_per_trial=2,\n",
    "#     directory='directory_for_model2',\n",
    "#     project_name='keras_tuner_model2',\n",
    "#     overwrite=False  # Enable resuming from checkpoints. If you set it equal to false, then the tuner will resume from the last checkpoint if one exists\n",
    "# )\n",
    "\n",
    "\n",
    "# # Define the early stopping callback\n",
    "# early_stopping_callback = EarlyStopping(\n",
    "#     monitor='val_loss',  # Monitor validation loss\n",
    "#     patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "#     restore_best_weights=True  # Restore model weights from the epoch with the best value\n",
    "# )\n",
    "\n",
    "\n",
    "# # Perform the hyperparameter search\n",
    "# tuner.search(train_generator, \n",
    "#              epochs=5, \n",
    "#              validation_data=val_generator,\n",
    "#              callbacks=[early_stopping_callback])  # Add the callback here\n",
    "\n",
    "# # Get the best model\n",
    "# best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "\n",
    "# #Print summary of the search space for the hyperparameters\n",
    "# tuner.search_space_summary()\n",
    "\n",
    "# # Get the best model\n",
    "# best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# # Trial 10 Complete [00h 00m 00s]\n",
    "\n",
    "# # Best val_accuracy So Far: 0.8330959677696228\n",
    "# # Total elapsed time: 00h 39m 44s\n",
    "\n",
    "# # Search: Running Trial #11\n",
    "\n",
    "# # Value             |Best Value So Far |Hyperparameter\n",
    "# # 112               |112               |conv_1_filters\n",
    "# # 4                 |2                 |num_layers\n",
    "# # 96                |80                |conv_2_filters\n",
    "# # 3                 |2                 |conv_2_kernel\n",
    "# # 3                 |2                 |pool_2_size\n",
    "# # 80                |112               |conv_3_filters\n",
    "# # 3                 |3                 |conv_3_kernel\n",
    "# # 3                 |2                 |pool_3_size\n",
    "# # 160               |224               |dense_1_units\n",
    "# # 0.0001            |0.001             |learning_rate\n",
    "# # 112               |32                |conv_4_filters\n",
    "# # 3                 |3                 |conv_4_kernel\n",
    "# # 3                 |2                 |pool_4_size\n",
    "# # 2                 |2                 |tuner/epochs\n",
    "# # 0                 |0                 |tuner/initial_epoch\n",
    "# # 4                 |4                 |tuner/bracket\n",
    "# # 0                 |0                 |tuner/round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5846/5846 [==============================] - 232s 40ms/step - loss: 0.4537 - accuracy: 0.7917 - val_loss: 0.4027 - val_accuracy: 0.8196\n",
      "Epoch 2/5\n",
      "5846/5846 [==============================] - 206s 35ms/step - loss: 0.3891 - accuracy: 0.8278 - val_loss: 0.3759 - val_accuracy: 0.8323\n",
      "Epoch 3/5\n",
      "5846/5846 [==============================] - 188s 32ms/step - loss: 0.3684 - accuracy: 0.8379 - val_loss: 0.3733 - val_accuracy: 0.8338\n",
      "Epoch 4/5\n",
      "5846/5846 [==============================] - 212s 36ms/step - loss: 0.3531 - accuracy: 0.8463 - val_loss: 0.3672 - val_accuracy: 0.8404\n",
      "Epoch 5/5\n",
      "5846/5846 [==============================] - 216s 37ms/step - loss: 0.3425 - accuracy: 0.8520 - val_loss: 0.3551 - val_accuracy: 0.8464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dfcfbba500>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "best_model = Sequential([\n",
    "    Conv2D(112, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(80, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(112, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(224, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer = Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def combine_generators(gen1, gen2):\n",
    "    while True:\n",
    "        yield next(gen1)\n",
    "        yield next(gen2)\n",
    "\n",
    "combined_generator = combine_generators(train_generator, val_generator) # I combined both the training and validation data so I can train the model on both, and then used the testing data as the validation data\n",
    "\n",
    "best_model.fit(combined_generator, steps_per_epoch=len(train_generator) + len(val_generator), validation_data = test_generator, epochs=5)\n",
    "\n",
    "# Took 17 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Information about Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model Summary:\n",
    "# best_model.summary()\n",
    "\n",
    "\n",
    "# # Get model configuration\n",
    "# config = best_model.get_config()\n",
    "# print(config)\n",
    "\n",
    "\n",
    "# # Get model weights\n",
    "# weights = best_model.get_weights()\n",
    "# for i, weight in enumerate(weights):\n",
    "#     print(f\"Weights of layer {i}: {weight}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Plot model architecture (visual representation)\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(best_model, to_file='model2_plot.png', show_shapes=True, show_layer_names=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 2. Score: 82.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796/1796 [==============================] - 60s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test\\\\00006537328c33e284c973d7b39d340809f7271b.tif', 'test\\\\0000ec92553fda4ce39889f9226ace43cae3364e.tif', 'test\\\\00024a6dee61f12f7856b0fc6be20bc7a48ba3d2.tif', 'test\\\\000253dfaa0be9d0d100283b22284ab2f6b643f6.tif', 'test\\\\000270442cc15af719583a8172c87cd2bd9c7746.tif']\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#Use the model from the keras tuner\n",
    "model2 = best_model\n",
    "\n",
    "# Make predictions\n",
    "predictions = model2.predict(real_test_generator, steps=len(real_test_generator), verbose=1)\n",
    "\n",
    "model2.save(\"model2_saved.h5\")\n",
    "\n",
    "# Get filenames (ordered list of image file names)\n",
    "filenames = real_test_generator.filenames\n",
    "print(filenames[:5])\n",
    "\n",
    "# Get the actual predictions, not the probabilities\n",
    "# If your model is a binary classifier, this will convert the probabilities into class predictions\n",
    "predicted_classes = [1 if prob > 0.5 else 0 for prob in predictions]\n",
    "\n",
    "# Create a DataFrame with filenames and predicted classes\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(filename).split('.')[0] for filename in filenames],  # Extract the id from the filename\n",
    "    'label': predicted_classes\n",
    "})\n",
    "\n",
    "# Save DataFrame to csv\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")\n",
    "\n",
    "# Took 1 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Maybe try GrayScale conversation\n",
    "### 2. Try Image cropping for 32x32px or 33x33px or no image cropping at all\n",
    "### 3. Try histogram equalization (part of data preprocessing)\n",
    "### 4. Find a way to make the images less blurry or find a way to make it not lose any pixels since each pixel is important\n",
    "### 5. Think about adding regularization into Keras tuner and maybe trying out different activation functions but adding activation functions will increase the run time a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
